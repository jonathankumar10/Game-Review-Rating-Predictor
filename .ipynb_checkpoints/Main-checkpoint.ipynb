{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading important packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = pd.read_csv('./datasets/2020-08-19.csv') '''not required for our project'''\n",
    "df2 = pd.read_csv('./datasets/bgg-15m-reviews.csv')\n",
    "# df3 = pd.read_csv('./datasets/games_detailed_info.csv') '''not required for our project''''''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping columns which are not required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del df2['Unnamed: 0']\n",
    "# del df2['ID']\n",
    "# del df2['name']\n",
    "# del df2['user']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping rows with NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df2.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2995023, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.     ,  9.8    ,  9.5    , ...,  7.98525,  9.03333,  5.767  ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['rating'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "decimals = pd.Series([0], index=['rating'])\n",
    "df = df.round(decimals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.,  9.,  8.,  7.,  6.,  5.,  4.,  3.,  2.,  1.,  0.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['rating'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rounding up the ratings for better efficieny of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rating_enhancement(rating):\n",
    "    if rating == 10.0:\n",
    "        return int(10)\n",
    "    elif rating >=9.5 and rating <10:\n",
    "        return int(10)\n",
    "    elif rating <9.5 and rating >=9:\n",
    "        return int(9)\n",
    "    elif rating >=8.5 and rating <9:\n",
    "        return int(9)\n",
    "    elif rating <8.5 and rating >=8:\n",
    "        return int(8)\n",
    "    elif rating >=7.5 and rating <8:\n",
    "        return int(8)\n",
    "    elif rating <7.5 and rating >=7:\n",
    "        return int(7)\n",
    "    elif rating >=6.5 and rating <7:\n",
    "        return int(7)\n",
    "    elif rating <6.5 and rating >=6:\n",
    "        return int(6)\n",
    "    elif rating >=5.5 and rating <6:\n",
    "        return int(6)\n",
    "    elif rating <5.5 and rating >=5:\n",
    "        return int(5)\n",
    "    elif rating >=4.5 and rating <5:\n",
    "        return int(5)\n",
    "    elif rating <4.5 and rating >=4:\n",
    "        return int(4)\n",
    "    elif rating >=3.5 and rating <4:\n",
    "        return int(4)\n",
    "    elif rating <3.5 and rating >=3:\n",
    "        return int(3)\n",
    "    elif rating >=2.5 and rating <3:\n",
    "        return int(3)\n",
    "    elif rating <2.5 and rating >=2:\n",
    "        return int(2)\n",
    "    elif rating >=1.5 and rating <2:\n",
    "        return int(2)\n",
    "    elif rating <1.5 and rating >=1:\n",
    "        return int(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['rating'] = df['rating'].apply(rating_enhancement)\n",
    "# df['rating'] = df['rating'].apply(rating_enhancement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.,  9.,  8.,  7.,  6.,  5.,  4.,  3.,  2.,  1.,  0.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['rating'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2995023, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.0</td>\n",
       "      <td>Hands down my favorite new game of BGG CON 200...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.0</td>\n",
       "      <td>I tend to either love or easily tire of co-op ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.0</td>\n",
       "      <td>This is an amazing co-op game.  I play mostly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10.0</td>\n",
       "      <td>Hey! I can finally rate this game I've been pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10.0</td>\n",
       "      <td>Love it- great fun with my son. 2 plays so far...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15823242</th>\n",
       "      <td>10.0</td>\n",
       "      <td>KS Collector's Bundle with a friend of mine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15823247</th>\n",
       "      <td>10.0</td>\n",
       "      <td>Belekokio Gerumo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15823253</th>\n",
       "      <td>10.0</td>\n",
       "      <td>Excelente!! lo mejor que probé.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15823264</th>\n",
       "      <td>8.0</td>\n",
       "      <td>Turn based preview looks very promising. The g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15823265</th>\n",
       "      <td>8.0</td>\n",
       "      <td>KS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2995023 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          rating                                            comment\n",
       "1           10.0  Hands down my favorite new game of BGG CON 200...\n",
       "2           10.0  I tend to either love or easily tire of co-op ...\n",
       "4           10.0  This is an amazing co-op game.  I play mostly ...\n",
       "5           10.0  Hey! I can finally rate this game I've been pl...\n",
       "8           10.0  Love it- great fun with my son. 2 plays so far...\n",
       "...          ...                                                ...\n",
       "15823242    10.0        KS Collector's Bundle with a friend of mine\n",
       "15823247    10.0                                Belekokio Gerumo...\n",
       "15823253    10.0                    Excelente!! lo mejor que probé.\n",
       "15823264     8.0  Turn based preview looks very promising. The g...\n",
       "15823265     8.0                                                 KS\n",
       "\n",
       "[2995023 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing in a pickle object as dataset is too large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  with open('./datasets/pickle_data/dataset_preclean.pkl', 'wb') as pickle_file:\n",
    "#         pickle.dump(df, pickle_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the pickle object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./datasets/pickle_data/dataset_preclean.pkl', 'rb') as pickle_file:\n",
    "        df = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_comments(text):\n",
    "    text = re.sub(r'<.*?>', ' ', text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"can not\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"'ve\", \" have\", text)\n",
    "    text = re.sub(r\"'ll\", \" will\", text)\n",
    "    text = re.sub(r\"'re\", \" are\", text)\n",
    "\n",
    "    text = re.sub(r\"[0-9]+\", ' ', text)\n",
    "    text = re.sub(r\"-\", ' ', text)\n",
    "    \n",
    "    \n",
    "    text = text.strip().lower()\n",
    "    \n",
    "\n",
    "    default_stop_words = set(stopwords.words('english'))\n",
    "    default_stop_words.difference_update({'no', 'not', 'nor', 'too', 'any'})\n",
    "    stop_words = default_stop_words.union({\"'m\", \"n't\", \"'d\", \"'re\", \"'s\",\n",
    "                                           'would','must',\"'ve\",\"'ll\",'may'})\n",
    "\n",
    "    word_list = word_tokenize(text)\n",
    "    filtered_list = [w for w in word_list if not w in stop_words]\n",
    "    text = ' '.join(filtered_list)\n",
    "    \n",
    "    text = re.sub(r\"'\", ' ', text)\n",
    "    \n",
    "   \n",
    "    filters='!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "    translate_dict = dict((i, \" \") for i in filters)\n",
    "    translate_map = str.maketrans(translate_dict)\n",
    "    text = text.translate(translate_map)\n",
    "    \n",
    "\n",
    "    text = ' '.join([w for w in text.split() if len(w)>1])\n",
    "\n",
    "    # Replace multiple space with one space\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    \n",
    "    text = ''.join(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 30min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['clean_comment'] = df['comment'].apply(clean_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words occuring less than 5 are: \n",
      "\n",
      "aparatoso      9\n",
      "fólia          9\n",
      "usen           9\n",
      "jinks          9\n",
      "kedv           9\n",
      "              ..\n",
      "narodowe       1\n",
      "rusht          1\n",
      "torkollhat     1\n",
      "inflaba        1\n",
      "selebrarion    1\n",
      "Length: 491238, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "freq_train1 = pd.Series(' '.join(df['clean_comment']).split()).value_counts()\n",
    "less_five_freq_train1 = freq_train1[(freq_train1 <10)]\n",
    "print('Words occuring less than 5 are: ')\n",
    "print('')\n",
    "print(less_five_freq_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 58.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['clean_comment'] = df['clean_comment'].apply(lambda x: ' '.join(x for x in x.split() if x not in less_five_freq_train1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NormalizeWithPOS(text):\n",
    "    # Lemmatization & Stemming according to POS tagging\n",
    "\n",
    "    word_list = word_tokenize(text)\n",
    "    rev = []\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    stemmer = PorterStemmer() \n",
    "    for word, tag in pos_tag(word_list):\n",
    "        if tag.startswith('J'):\n",
    "            w = lemmatizer.lemmatize(word, pos='a')\n",
    "        elif tag.startswith('V'):\n",
    "            w = lemmatizer.lemmatize(word, pos='v')\n",
    "        elif tag.startswith('N'):\n",
    "            w = lemmatizer.lemmatize(word, pos='n')\n",
    "        elif tag.startswith('R'):\n",
    "            w = lemmatizer.lemmatize(word, pos='r')\n",
    "        else:\n",
    "            w = word\n",
    "        w = stemmer.stem(w)\n",
    "        rev.append(w)\n",
    "    review = ' '.join(rev)\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 51min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['clean_comment'] = df['clean_comment'].apply(NormalizeWithPOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.0</td>\n",
       "      <td>Hands down my favorite new game of BGG CON 200...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.0</td>\n",
       "      <td>I tend to either love or easily tire of co-op ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.0</td>\n",
       "      <td>This is an amazing co-op game.  I play mostly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10.0</td>\n",
       "      <td>Hey! I can finally rate this game I've been pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10.0</td>\n",
       "      <td>Love it- great fun with my son. 2 plays so far...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15823242</th>\n",
       "      <td>10.0</td>\n",
       "      <td>KS Collector's Bundle with a friend of mine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15823247</th>\n",
       "      <td>10.0</td>\n",
       "      <td>Belekokio Gerumo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15823253</th>\n",
       "      <td>10.0</td>\n",
       "      <td>Excelente!! lo mejor que probé.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15823264</th>\n",
       "      <td>8.0</td>\n",
       "      <td>Turn based preview looks very promising. The g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15823265</th>\n",
       "      <td>8.0</td>\n",
       "      <td>KS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2995023 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          rating                                            comment\n",
       "1           10.0  Hands down my favorite new game of BGG CON 200...\n",
       "2           10.0  I tend to either love or easily tire of co-op ...\n",
       "4           10.0  This is an amazing co-op game.  I play mostly ...\n",
       "5           10.0  Hey! I can finally rate this game I've been pl...\n",
       "8           10.0  Love it- great fun with my son. 2 plays so far...\n",
       "...          ...                                                ...\n",
       "15823242    10.0        KS Collector's Bundle with a friend of mine\n",
       "15823247    10.0                                Belekokio Gerumo...\n",
       "15823253    10.0                    Excelente!! lo mejor que probé.\n",
       "15823264     8.0  Turn based preview looks very promising. The g...\n",
       "15823265     8.0                                                 KS\n",
       "\n",
       "[2995023 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickling the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./datasets/pickle_data/dataset.pkl', 'wb') as pickle_file:\n",
    "#         pickle.dump(df, pickle_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the cleaned data object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./datasets/pickle_data/dataset.pkl', 'rb') as pickle_file:\n",
    "        dataset = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rating:  0.0 rating num: 9\n",
      "rating:  1.0 rating num: 23513\n",
      "rating:  2.0 rating num: 45046\n",
      "rating:  3.0 rating num: 79101\n",
      "rating:  4.0 rating num: 151068\n",
      "rating:  5.0 rating num: 242647\n",
      "rating:  6.0 rating num: 587568\n",
      "rating:  7.0 rating num: 650409\n",
      "rating:  8.0 rating num: 751962\n",
      "rating:  9.0 rating num: 279371\n",
      "rating:  10.0 rating num: 184329\n"
     ]
    }
   ],
   "source": [
    "rating_num_set = {}\n",
    "for rating in (10-dataset['rating'].unique()):\n",
    "    new_comment_rating = dataset.loc[dataset['rating'] >= (rating - 0.5)]\n",
    "    new_comment_rating = new_comment_rating.loc[new_comment_rating['rating'] <= (rating + 0.5)]\n",
    "    new_comment_rating = new_comment_rating.sample(frac = 1).reset_index(drop = True)\n",
    "    rating_num_set[rating] = new_comment_rating\n",
    "\n",
    "for rating in rating_num_set:\n",
    "    print(\"rating: \", rating, \"rating num:\",  len(rating_num_set[rating]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAD4CAYAAADy46FuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAWsklEQVR4nO3df4xd5X3n8fenOKUkXYhNDGJttKbCSgtISYpl6EaqunHXdpUq5g+QJlKDVXnlFaLdZFWpMv3HWpAlkFalRVqQUHAxNBvwuomwkiXUMo2qlZBhSNglQJBnAwUvFE8zDmFbQdb0u3/MM/L1ZHjmjpkfeHi/pKtzzvec57nPkS1/fM5z7r2pKiRJei+/sNQDkCR9sBkUkqQug0KS1GVQSJK6DApJUteKpR7AfPvEJz5R69atW+phSNJZ5emnn/6Hqlo9075lFxTr1q1jdHR0qYchSWeVJH/3Xvu89SRJ6jIoJEldBoUkqcugkCR1GRSSpC6DQpLUZVBIkroMCklSl0EhSepadp/MlnR2W7fr2wvW98u3f37B+l7OvKKQJHUZFJKkLoNCktRlUEiSugwKSVKXQSFJ6jIoJEldBoUkqcugkCR1GRSSpK5ZgyLJJ5M8M/D6aZKvJFmV5FCSo225cqDNLUnGkryYZMtA/eokz7Z9dyVJq5+b5OFWP5Jk3UCb7e09jibZPr+nL0mazaxBUVUvVtWnq+rTwNXAPwHfBHYBh6tqPXC4bZPkCmAEuBLYCtyd5JzW3T3ATmB9e21t9R3Aiaq6HLgTuKP1tQrYDVwDbAR2DwaSJGnhzfXW0ybgf1fV3wHbgH2tvg+4rq1vAx6qqneq6iVgDNiY5BLg/Kp6oqoKeGBam6m+DgCb2tXGFuBQVU1U1QngEKfCRZK0COYaFCPA19v6xVX1OkBbXtTqa4BXB9oca7U1bX16/bQ2VXUSeBO4sNPXaZLsTDKaZHR8fHyOpyRJ6hk6KJL8IvAF4L/NdugMterUz7TNqULVvVW1oao2rF69epbhSZLmYi5XFL8DfK+q3mjbb7TbSbTl8VY/Blw60G4t8Fqrr52hflqbJCuAC4CJTl+SpEUyl6D4IqduOwEcBKaeQtoOPDJQH2lPMl3G5KT1k+321FtJrm3zDzdOazPV1/XA420e4zFgc5KVbRJ7c6tJkhbJUL9wl+SjwL8F/v1A+XZgf5IdwCvADQBV9VyS/cDzwEng5qp6t7W5CbgfOA94tL0A7gMeTDLG5JXESOtrIsltwFPtuFurauIMzlOSdIaGCoqq+icmJ5cHaz9m8imomY7fA+yZoT4KXDVD/W1a0Mywby+wd5hxSpLmn5/MliR1GRSSpC6DQpLUZVBIkroMCklSl0EhSeoyKCRJXQaFJKlrqA/cSfrwWrfr2wvS78u3f35B+tX884pCktRlUEiSugwKSVKXQSFJ6jIoJEldBoUkqcugkCR1GRSSpC6DQpLUNVRQJPl4kgNJfpjkhSS/kWRVkkNJjrblyoHjb0kyluTFJFsG6lcnebbtuytJWv3cJA+3+pEk6wbabG/vcTTJ9vk7dUnSMIa9ovhz4DtV9avAp4AXgF3A4apaDxxu2yS5AhgBrgS2AncnOaf1cw+wE1jfXltbfQdwoqouB+4E7mh9rQJ2A9cAG4Hdg4EkSVp4swZFkvOB3wTuA6iqn1XVT4BtwL522D7gura+DXioqt6pqpeAMWBjkkuA86vqiaoq4IFpbab6OgBsalcbW4BDVTVRVSeAQ5wKF0nSIhjmiuJXgHHgL5J8P8lXk3wMuLiqXgdoy4va8WuAVwfaH2u1NW19ev20NlV1EngTuLDTlyRpkQwTFCuAXwfuqarPAP9Iu830HjJDrTr1M21z6g2TnUlGk4yOj493hiZJmqthguIYcKyqjrTtA0wGxxvtdhJteXzg+EsH2q8FXmv1tTPUT2uTZAVwATDR6es0VXVvVW2oqg2rV68e4pQkScOaNSiq6u+BV5N8spU2Ac8DB4Gpp5C2A4+09YPASHuS6TImJ62fbLen3kpybZt/uHFam6m+rgceb/MYjwGbk6xsk9ibW02StEiG/eGiPwS+luQXgR8Bv89kyOxPsgN4BbgBoKqeS7KfyTA5CdxcVe+2fm4C7gfOAx5tL5icKH8wyRiTVxIjra+JJLcBT7Xjbq2qiTM8V2lZ8IeEtNiGCoqqegbYMMOuTe9x/B5gzwz1UeCqGepv04Jmhn17gb3DjFOSNP/8ZLYkqcugkCR1GRSSpC6DQpLUZVBIkroMCklSl0EhSeoyKCRJXQaFJKnLoJAkdRkUkqQug0KS1GVQSJK6DApJUpdBIUnqMigkSV0GhSSpy6CQJHUZFJKkrqGCIsnLSZ5N8kyS0VZbleRQkqNtuXLg+FuSjCV5McmWgfrVrZ+xJHclSaufm+ThVj+SZN1Am+3tPY4m2T5fJy5JGs5crij+TVV9uqo2tO1dwOGqWg8cbtskuQIYAa4EtgJ3JzmntbkH2Amsb6+trb4DOFFVlwN3Ane0vlYBu4FrgI3A7sFAkiQtvPdz62kbsK+t7wOuG6g/VFXvVNVLwBiwMcklwPlV9URVFfDAtDZTfR0ANrWrjS3AoaqaqKoTwCFOhYskaREMGxQF/HWSp5PsbLWLq+p1gLa8qNXXAK8OtD3Wamva+vT6aW2q6iTwJnBhp6/TJNmZZDTJ6Pj4+JCnJEkaxoohj/tsVb2W5CLgUJIfdo7NDLXq1M+0zalC1b3AvQAbNmz4uf2SpDM31BVFVb3WlseBbzI5X/BGu51EWx5vhx8DLh1ovhZ4rdXXzlA/rU2SFcAFwESnL0nSIpk1KJJ8LMm/mFoHNgM/AA4CU08hbQceaesHgZH2JNNlTE5aP9luT72V5No2/3DjtDZTfV0PPN7mMR4DNidZ2SaxN7eaJGmRDHPr6WLgm+1J1hXAf62q7yR5CtifZAfwCnADQFU9l2Q/8DxwEri5qt5tfd0E3A+cBzzaXgD3AQ8mGWPySmKk9TWR5DbgqXbcrVU18T7OV5I0R7MGRVX9CPjUDPUfA5veo80eYM8M9VHgqhnqb9OCZoZ9e4G9s41TkrQw/GS2JKnLoJAkdRkUkqQug0KS1GVQSJK6DApJUpdBIUnqMigkSV0GhSSpy6CQJHUZFJKkLoNCktRlUEiSugwKSVKXQSFJ6jIoJEldBoUkqcugkCR1DR0USc5J8v0k32rbq5IcSnK0LVcOHHtLkrEkLybZMlC/Osmzbd9daT/EneTcJA+3+pEk6wbabG/vcTTJ9vk4aUnS8OZyRfFl4IWB7V3A4apaDxxu2yS5AhgBrgS2AncnOae1uQfYCaxvr62tvgM4UVWXA3cCd7S+VgG7gWuAjcDuwUCSJC28oYIiyVrg88BXB8rbgH1tfR9w3UD9oap6p6peAsaAjUkuAc6vqieqqoAHprWZ6usAsKldbWwBDlXVRFWdAA5xKlwkSYtg2CuKPwP+GPjngdrFVfU6QFte1OprgFcHjjvWamva+vT6aW2q6iTwJnBhpy9J0iKZNSiS/C5wvKqeHrLPzFCrTv1M2wyOcWeS0SSj4+PjQw5TkjSMYa4oPgt8IcnLwEPA55L8JfBGu51EWx5vxx8DLh1ovxZ4rdXXzlA/rU2SFcAFwESnr9NU1b1VtaGqNqxevXqIU5IkDWvWoKiqW6pqbVWtY3KS+vGq+j3gIDD1FNJ24JG2fhAYaU8yXcbkpPWT7fbUW0mubfMPN05rM9XX9e09CngM2JxkZZvE3txqkqRFsuJ9tL0d2J9kB/AKcANAVT2XZD/wPHASuLmq3m1tbgLuB84DHm0vgPuAB5OMMXklMdL6mkhyG/BUO+7Wqpp4H2OWJM3RnIKiqr4LfLet/xjY9B7H7QH2zFAfBa6aof42LWhm2LcX2DuXcUqS5o+fzJYkdRkUkqQug0KS1GVQSJK6DApJUpdBIUnqMigkSV0GhSSpy6CQJHUZFJKkLoNCktRlUEiSugwKSVKXQSFJ6jIoJEldBoUkqcugkCR1GRSSpC6DQpLUNWtQJPmlJE8m+Z9Jnkvyn1p9VZJDSY625cqBNrckGUvyYpItA/Wrkzzb9t2VJK1+bpKHW/1IknUDbba39ziaZPt8nrwkaXbDXFG8A3yuqj4FfBrYmuRaYBdwuKrWA4fbNkmuAEaAK4GtwN1Jzml93QPsBNa319ZW3wGcqKrLgTuBO1pfq4DdwDXARmD3YCBJkhberEFRk/5v2/xIexWwDdjX6vuA69r6NuChqnqnql4CxoCNSS4Bzq+qJ6qqgAemtZnq6wCwqV1tbAEOVdVEVZ0ADnEqXCRJi2CoOYok5yR5BjjO5D/cR4CLq+p1gLa8qB2+Bnh1oPmxVlvT1qfXT2tTVSeBN4ELO31NH9/OJKNJRsfHx4c5JUnSkIYKiqp6t6o+Daxl8urgqs7hmamLTv1M2wyO796q2lBVG1avXt0ZmiRprub01FNV/QT4LpO3f95ot5Noy+PtsGPApQPN1gKvtfraGeqntUmyArgAmOj0JUlaJMM89bQ6ycfb+nnAbwM/BA4CU08hbQceaesHgZH2JNNlTE5aP9luT72V5No2/3DjtDZTfV0PPN7mMR4DNidZ2SaxN7eaJGmRrBjimEuAfe3JpV8A9lfVt5I8AexPsgN4BbgBoKqeS7IfeB44CdxcVe+2vm4C7gfOAx5tL4D7gAeTjDF5JTHS+ppIchvwVDvu1qqaeD8nLEmam1mDoqr+F/CZGeo/Bja9R5s9wJ4Z6qPAz81vVNXbtKCZYd9eYO9s45QkLYxhrigkadlat+vbC9b3y7d/fsH6Xkx+hYckqcugkCR1GRSSpC6DQpLUZVBIkroMCklSl0EhSeoyKCRJXQaFJKnLT2ZL79NCfbJ3uXyqV2c/rygkSV0GhSSpy6CQJHUZFJKkLoNCktRlUEiSugwKSVKXQSFJ6po1KJJcmuRvkryQ5LkkX271VUkOJTnalisH2tySZCzJi0m2DNSvTvJs23dXkrT6uUkebvUjSdYNtNne3uNoku3zefKSpNkNc0VxEvijqvo14Frg5iRXALuAw1W1Hjjctmn7RoArga3A3UnOaX3dA+wE1rfX1lbfAZyoqsuBO4E7Wl+rgN3ANcBGYPdgIEmSFt6sQVFVr1fV99r6W8ALwBpgG7CvHbYPuK6tbwMeqqp3quolYAzYmOQS4PyqeqKqCnhgWpupvg4Am9rVxhbgUFVNVNUJ4BCnwkWStAjmNEfRbgl9BjgCXFxVr8NkmAAXtcPWAK8ONDvWamva+vT6aW2q6iTwJnBhp6/p49qZZDTJ6Pj4+FxOSZI0i6GDIskvA38FfKWqfto7dIZadepn2uZUoereqtpQVRtWr17dGZokaa6GCookH2EyJL5WVd9o5Tfa7STa8nirHwMuHWi+Fnit1dfOUD+tTZIVwAXARKcvSdIiGeappwD3AS9U1Z8O7DoITD2FtB14ZKA+0p5kuozJSesn2+2pt5Jc2/q8cVqbqb6uBx5v8xiPAZuTrGyT2JtbTZK0SIb5PYrPAl8Cnk3yTKv9CXA7sD/JDuAV4AaAqnouyX7geSafmLq5qt5t7W4C7gfOAx5tL5gMogeTjDF5JTHS+ppIchvwVDvu1qqaOMNzlaQlt1C/XwIL9xsmswZFVf0PZp4rANj0Hm32AHtmqI8CV81Qf5sWNDPs2wvsnW2ckqSF4SezJUldBoUkqcugkCR1GRSSpC6DQpLUZVBIkroMCklS1zAfuJPOGmfjh5mkDzqvKCRJXQaFJKnLoJAkdRkUkqQug0KS1GVQSJK6DApJUpdBIUnqMigkSV0GhSSpa9agSLI3yfEkPxiorUpyKMnRtlw5sO+WJGNJXkyyZaB+dZJn2767kqTVz03ycKsfSbJuoM329h5Hk2yfr5OWJA1vmCuK+4Gt02q7gMNVtR443LZJcgUwAlzZ2tyd5JzW5h5gJ7C+vab63AGcqKrLgTuBO1pfq4DdwDXARmD3YCBJkhbHrEFRVX8LTEwrbwP2tfV9wHUD9Yeq6p2qegkYAzYmuQQ4v6qeqKoCHpjWZqqvA8CmdrWxBThUVRNVdQI4xM8HliRpgZ3pHMXFVfU6QFte1OprgFcHjjvWamva+vT6aW2q6iTwJnBhp6+fk2RnktEko+Pj42d4SpKkmcz3ZHZmqFWnfqZtTi9W3VtVG6pqw+rVq4caqCRpOGcaFG+020m05fFWPwZcOnDcWuC1Vl87Q/20NklWABcweavrvfqSJC2iMw2Kg8DUU0jbgUcG6iPtSabLmJy0frLdnnorybVt/uHGaW2m+roeeLzNYzwGbE6ysk1ib241SdIimvUX7pJ8Hfgt4BNJjjH5JNLtwP4kO4BXgBsAquq5JPuB54GTwM1V9W7r6iYmn6A6D3i0vQDuAx5MMsbklcRI62siyW3AU+24W6tq+qS6JGmBzRoUVfXF99i16T2O3wPsmaE+Clw1Q/1tWtDMsG8vsHe2MUqSFo6/ma0F5W9YS2c/v8JDktRlUEiSugwKSVKXQSFJ6jIoJEldBoUkqcugkCR1GRSSpC4/cPch4wfgJM2VVxSSpC6DQpLUZVBIkroMCklSl0EhSeryqacltlBPIfkEkqT54hWFJKnLoJAkdRkUkqSusyIokmxN8mKSsSS7lno8kvRh8oEPiiTnAP8F+B3gCuCLSa5Y2lFJ0ofHBz4ogI3AWFX9qKp+BjwEbFviMUnSh0aqaqnH0JXkemBrVf27tv0l4Jqq+oOBY3YCO9vmJ4EXF2l4nwD+YZHeayks9/OD5X+Ont/Zb7HO8V9V1eqZdpwNn6PIDLXT0q2q7gXuXZzhnJJktKo2LPb7Lpblfn6w/M/R8zv7fRDO8Wy49XQMuHRgey3w2hKNRZI+dM6GoHgKWJ/ksiS/CIwAB5d4TJL0ofGBv/VUVSeT/AHwGHAOsLeqnlviYU1Z9Ntdi2y5nx8s/3P0/M5+S36OH/jJbEnS0jobbj1JkpaQQSFJ6jIozsBy/0qRJJcm+ZskLyR5LsmXl3pMCyHJOUm+n+RbSz2WhZDk40kOJPlh+7P8jaUe03xK8h/b388fJPl6kl9a6jG9X0n2Jjme5AcDtVVJDiU52pYrF3tcBsUcfUi+UuQk8EdV9WvAtcDNy/AcAb4MvLDUg1hAfw58p6p+FfgUy+hck6wB/gOwoaquYvJBl5GlHdW8uB/YOq22CzhcVeuBw217URkUc7fsv1Kkql6vqu+19beY/AdmzdKOan4lWQt8HvjqUo9lISQ5H/hN4D6AqvpZVf1kaUc171YA5yVZAXyUZfD5qqr6W2BiWnkbsK+t7wOuW9RBYVCciTXAqwPbx1hm/4gOSrIO+AxwZGlHMu/+DPhj4J+XeiAL5FeAceAv2u21ryb52FIPar5U1f8B/jPwCvA68GZV/fXSjmrBXFxVr8Pkf+KAixZ7AAbF3M36lSLLRZJfBv4K+EpV/XSpxzNfkvwucLyqnl7qsSygFcCvA/dU1WeAf2QJblkslHaffhtwGfAvgY8l+b2lHdXyZVDM3YfiK0WSfITJkPhaVX1jqcczzz4LfCHJy0zeOvxckr9c2iHNu2PAsaqauhI8wGRwLBe/DbxUVeNV9f+AbwD/eonHtFDeSHIJQFseX+wBGBRzt+y/UiRJmLy3/UJV/elSj2e+VdUtVbW2qtYx+ef3eFUtq/+NVtXfA68m+WQrbQKeX8IhzbdXgGuTfLT9fd3EMpqsn+YgsL2tbwceWewBfOC/wuOD5gP+lSLz5bPAl4BnkzzTan9SVf99CcekuftD4GvtPzQ/An5/icczb6rqSJIDwPeYfErv+3wAvuri/UrydeC3gE8kOQbsBm4H9ifZwWRA3rDo4/IrPCRJPd56kiR1GRSSpC6DQpLUZVBIkroMCklSl0EhSeoyKCRJXf8fQbQsgy7ef9kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rating_list = []\n",
    "for rating in rating_num_set: \n",
    "    rating_list.append(len(rating_num_set[rating]))\n",
    "plt.bar(range(len(rating_list)), rating_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A review example of dataset before cleaning:\n",
      "Hands down my favorite new game of BGG CON 2007.  We played it 5 times in a row -- it's just that good.  Too bad Pandemic won't be in stores until January of 2008.  If you like pure coöp games (Lord of the Rings, Feurio, etc.), this should be right up your alley.  Having 5 roles to choose from gives the game some extra variability.  Also, once you get good you can ramp up the difficulty by adding more Epidemic cards.  9 -> 10\n",
      "\n",
      "clean_text:\n",
      "hand favorit new game bgg con play time row good too bad pandem not store januari like pure coöp game lord ring feurio etc right alley role choos give game extra variabl also get good ramp difficulti add epidem card\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"A review example of dataset before cleaning:\")\n",
    "print(dataset.iloc[0]['comment'], end='\\n\\n')\n",
    "\n",
    "print(\"clean_text:\")\n",
    "print(dataset.iloc[0]['clean_comment'], end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataset['clean_comment'], dataset['rating'], test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.index = [x for x in range(1, len(X_train.values)+1)]\n",
    "# X_test.index = [x for x in range(1, len(X_test.values)+1)]\n",
    "# y_train.index = [x for x in range(1, len(y_train.values)+1)]\n",
    "# y_test.index = [x for x in range(1, len(y_test.values)+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./datasets/pickle_data/X_train.pkl', 'wb') as pickle_file:\n",
    "#     pickle.dump(X_train, pickle_file)\n",
    "# with open('./datasets/pickle_data/X_test.pkl', 'wb') as pickle_file:\n",
    "#     pickle.dump(X_test, pickle_file)\n",
    "# with open('./datasets/pickle_data/y_train.pkl', 'wb') as pickle_file:\n",
    "#     pickle.dump(y_train, pickle_file)\n",
    "# with open('./datasets/pickle_data/y_test.pkl', 'wb') as pickle_file:\n",
    "#     pickle.dump(y_test, pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./datasets/pickle_data/X_train.pkl', 'rb') as pickle_file:\n",
    "    X_train = pickle.load(pickle_file)\n",
    "with open('./datasets/pickle_data/X_test.pkl', 'rb') as pickle_file:\n",
    "    X_test = pickle.load(pickle_file)\n",
    "with open('./datasets/pickle_data/y_train.pkl', 'rb') as pickle_file:\n",
    "    y_train = pickle.load(pickle_file)\n",
    "with open('./datasets/pickle_data/y_test.pkl', 'rb') as pickle_file:\n",
    "    y_test = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "training_features = vectorizer.fit_transform(X_train)\n",
    "testing_features = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./datasets/pickle_data/vectorizer.pkl', 'wb') as pickle_file:\n",
    "    pickle.dump(vectorizer, pickle_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickling the bow object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./datasets/pickle_data/training_features.pkl', 'wb') as pickle_file:\n",
    "    pickle.dump(training_features, pickle_file)\n",
    "with open('./datasets/pickle_data/testing_features.pkl', 'wb') as pickle_file:\n",
    "    pickle.dump(testing_features, pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./datasets/pickle_data/training_features.pkl', 'rb') as pickle_file:\n",
    "    training_features = pickle.load(pickle_file)\n",
    "with open('./datasets/pickle_data/testing_features.pkl', 'rb') as pickle_file:\n",
    "    testing_features = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2246267, 50737)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(748756, 50737)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.06 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model.fit(training_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_total = model.predict(testing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.,  7.,  8., ...,  4.,  6., 10.])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1         8.0\n",
       "2         8.0\n",
       "3         6.0\n",
       "4         5.0\n",
       "5         5.0\n",
       "         ... \n",
       "748752    3.0\n",
       "748753    8.0\n",
       "748754    5.0\n",
       "748755    7.0\n",
       "748756    8.0\n",
       "Name: rating, Length: 748756, dtype: float64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 1.0, 'class_prior': None, 'fit_prior': True}\n"
     ]
    }
   ],
   "source": [
    "print(model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = np.linspace(0,1,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': array([0.        , 0.01010101, 0.02020202, 0.03030303, 0.04040404,\n",
      "       0.05050505, 0.06060606, 0.07070707, 0.08080808, 0.09090909,\n",
      "       0.1010101 , 0.11111111, 0.12121212, 0.13131313, 0.14141414,\n",
      "       0.15151515, 0.16161616, 0.17171717, 0.18181818, 0.19191919,\n",
      "       0.2020202 , 0.21212121, 0.22222222, 0.23232323, 0.24242424,\n",
      "       0.25252525, 0.26262626, 0.27272727, 0.28282828, 0.29292929,\n",
      "       0.3030303 , 0.31313131, 0.32323232, 0.33333333, 0.34343434,\n",
      "       0.35353535, 0.36363636, 0.37373737, 0.38383838, 0.39393939,\n",
      "       0.4040404 , 0.41414141, 0.42424242, 0.43434343, 0.44444444,\n",
      "       0.45454545, 0.46464646, 0.47474747, 0.48484848, 0.49494949,\n",
      "       0.50505051, 0.51515152, 0.52525253, 0.53535354, 0.54545455,\n",
      "       0.55555556, 0.56565657, 0.57575758, 0.58585859, 0.5959596 ,\n",
      "       0.60606061, 0.61616162, 0.62626263, 0.63636364, 0.64646465,\n",
      "       0.65656566, 0.66666667, 0.67676768, 0.68686869, 0.6969697 ,\n",
      "       0.70707071, 0.71717172, 0.72727273, 0.73737374, 0.74747475,\n",
      "       0.75757576, 0.76767677, 0.77777778, 0.78787879, 0.7979798 ,\n",
      "       0.80808081, 0.81818182, 0.82828283, 0.83838384, 0.84848485,\n",
      "       0.85858586, 0.86868687, 0.87878788, 0.88888889, 0.8989899 ,\n",
      "       0.90909091, 0.91919192, 0.92929293, 0.93939394, 0.94949495,\n",
      "       0.95959596, 0.96969697, 0.97979798, 0.98989899, 1.        ]), 'fit_prior': [True, False]}\n"
     ]
    }
   ],
   "source": [
    "random_grid = {'alpha': alpha,\n",
    "              'fit_prior': [True,False]}\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_random = RandomizedSearchCV(estimator = model, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   13.4s\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  2.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=MultinomialNB(), n_iter=100, n_jobs=-1,\n",
       "                   param_distributions={'alpha': array([0.        , 0.01010101, 0.02020202, 0.03030303, 0.04040404,\n",
       "       0.05050505, 0.06060606, 0.07070707, 0.08080808, 0.09090909,\n",
       "       0.1010101 , 0.11111111, 0.12121212, 0.13131313, 0.14141414,\n",
       "       0.15151515, 0.16161616, 0.17171717, 0.18181818, 0.19191919,\n",
       "       0.2020202 , 0.21212121, 0.22222222,...\n",
       "       0.70707071, 0.71717172, 0.72727273, 0.73737374, 0.74747475,\n",
       "       0.75757576, 0.76767677, 0.77777778, 0.78787879, 0.7979798 ,\n",
       "       0.80808081, 0.81818182, 0.82828283, 0.83838384, 0.84848485,\n",
       "       0.85858586, 0.86868687, 0.87878788, 0.88888889, 0.8989899 ,\n",
       "       0.90909091, 0.91919192, 0.92929293, 0.93939394, 0.94949495,\n",
       "       0.95959596, 0.96969697, 0.97979798, 0.98989899, 1.        ]),\n",
       "                                        'fit_prior': [True, False]},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_random.fit(training_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.9797979797979799)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_random.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_features, test_labels):\n",
    "    predictions = model.predict(test_features)\n",
    "    errors = abs(predictions - test_labels)\n",
    "    MSE = mean_squared_error(test_labels,predictions)\n",
    "    accuracy = accuracy_score(test_labels,predictions)*100\n",
    "    print('Model Performance')\n",
    "    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
    "    print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
    "    print('MSE = {:0.2f}.'.format(MSE))\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "Average Error: 1.2601 degrees.\n",
      "Accuracy = 30.50%.\n",
      "MSE = 3.30.\n"
     ]
    }
   ],
   "source": [
    "base_model = MultinomialNB(alpha = 0.5)\n",
    "base_model.fit(training_features, y_train)\n",
    "base_accuracy = evaluate(base_model, testing_features, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "Average Error: 1.2516 degrees.\n",
      "Accuracy = 30.64%.\n",
      "MSE = 3.26.\n"
     ]
    }
   ],
   "source": [
    "best_random = model_random.best_estimator_\n",
    "random_accuracy = evaluate(best_random, testing_features, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model post Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_multinomial = MultinomialNB(alpha = 0.9797979797979799)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.16 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.9797979797979799)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "final_model_multinomial.fit(training_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_total = final_model_multinomial.predict(testing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2568513641293024"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test,predict_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./datasets/pickle_data/final_model_multinomial.pkl', 'wb') as pickle_file:\n",
    "    pickle.dump(final_model_multinomial, pickle_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Input and model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter a review: worst game ever\n"
     ]
    }
   ],
   "source": [
    "Input = input('Please enter a review: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 30.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "Input = [Input]\n",
    "input_df = DataFrame(Input,columns=['comment'])\n",
    "input_df['clean_comment'] = input_df['comment'].apply(clean_comments)\n",
    "input_df['clean_comment'] = input_df['clean_comment'].apply(NormalizeWithPOS)\n",
    "input_testing_features = vectorizer.transform(input_df['clean_comment'])\n",
    "predict = model.predict(input_testing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_randomforest = RandomForestClassifier(bootstrap=False,n_estimators=100, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "model_randomforest.fit(training_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_total = model_randomforest.predict(testing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_randomforest.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 1000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the random grid\n",
    "random_grid_forest = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "print(random_grid_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_random_forest = RandomizedSearchCV(estimator = model_randomforest, param_distributions = random_grid_forest, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_random_forest.fit(training_features, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_forest = model_randomforest\n",
    "base_accuracy = evaluate(base_model_forest, testing_features, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_random_forest = model_random.best_estimator_\n",
    "random_accuracy = evaluate(best_random_forest, testing_features, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model post Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
